# LEO Optima - User Onboarding Guide

**Complete walkthrough for new users: From installation to monitoring savings**

---

## üéØ What is LEO Optima?

LEO Optima is a **cost-reduction engine for LLM API calls**. It sits between your application and OpenAI (or other LLM providers) and automatically:

- **Reduces costs by 60-80%** through intelligent caching and optimization
- **Maintains response quality** using semantic analysis and confidence scoring
- **Provides real-time monitoring** via a professional dashboard
- **Supports multiple users** with isolated analytics and quotas

---

## üìã Prerequisites

Before you start, ensure you have:

1. **Docker & Docker Compose** installed (recommended)
   - Or Python 3.11+ if running manually
2. **OpenAI API Key** (from https://platform.openai.com/api-keys)
3. **Git** installed to clone the repository
4. **5-10 minutes** to get everything running

---

## üöÄ Step 1: Clone & Setup (2 minutes)

### 1.1 Clone the Repository

```bash
git clone https://github.com/BADJAB22/leo-optima.git
cd leo-optima
```

### 1.2 Configure Environment Variables

```bash
# Copy the example environment file
cp .env.example .env

# Open .env in your editor and add your keys
nano .env  # or use your preferred editor
```

**Edit `.env` and add these values:**

```bash
# Your OpenAI API Key (from https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-actual-openai-key-here

# A secret key for LEO Optima (create a strong random string)
LEO_API_KEY=your_super_secret_leo_api_key_12345

# Optional: Redis configuration (auto-configured in Docker)
REDIS_URL=redis://localhost:6379

# Optional: Storage path
STORAGE_PATH=/app/leo_storage
```

**Example of a complete `.env` file:**

```bash
OPENAI_API_KEY=sk-proj-abc123xyz789...
LEO_API_KEY=leo_prod_secret_key_2024
REDIS_URL=redis://redis:6379
STORAGE_PATH=/app/leo_storage
```

---

## üê≥ Step 2: Start LEO Optima (1 minute)

### Option A: Docker Compose (Recommended)

```bash
# Build and start all services
docker compose up --build -d

# Check if services are running
docker compose ps

# View logs
docker compose logs -f leo-optima
```

**Expected output:**

```
leo-optima  | INFO:     Application startup complete
redis       | Ready to accept connections
```

### Option B: Manual Setup (Python)

```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY="sk-your-key"
export LEO_API_KEY="your_secret_key"

# Start the server
python proxy_server.py
```

**Expected output:**

```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

---

## üé® Step 3: Access the Dashboard (1 minute)

Once LEO Optima is running, open your browser:

### Dashboard URL: http://localhost:3000

You'll see a professional dashboard with:

**Top Section - Key Metrics (4 Cards)**
- **Total Cost Saved**: How much money you've saved (e.g., $1,247.50)
- **Tokens Optimized**: Total tokens reduced (e.g., 2.8M)
- **Cache Hit Rate**: % of requests served from cache (e.g., 42%)
- **Requests Processed**: Total requests today (e.g., 12,847)

**Middle Section - Charts**
- **Cost & Token Savings Chart**: Line graph showing savings over time
- **Request Routes Pie Chart**: Shows distribution of Cache (42%), Fast (38%), Consensus (20%)

**Bottom Section - Multi-Tenant Performance**
- Table showing each tenant's requests, savings, and cache hit rate

---

## üîå Step 4: Connect Your Application (2 minutes)

LEO Optima acts as a **drop-in replacement** for OpenAI. Just change your code to point to LEO instead.

### Before (Direct OpenAI)

```python
from openai import OpenAI

client = OpenAI(api_key="sk-your-openai-key")

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### After (Using LEO Optima)

```python
from openai import OpenAI
import os

# Point to LEO Optima instead of OpenAI
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),  # Still use your OpenAI key
    base_url="http://localhost:8000/v1",  # Point to LEO
    default_headers={
        "X-API-Key": os.getenv("LEO_API_KEY")  # LEO's secret key
    }
)

# Everything else stays the same!
response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

**That's it! Your application now uses LEO Optima automatically.**

### For Node.js

```javascript
const OpenAI = require("openai");

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  baseURL: "http://localhost:8000/v1",
  defaultHeaders: {
    "X-API-Key": process.env.LEO_API_KEY
  }
});

const response = await client.chat.completions.create({
  model: "gpt-4",
  messages: [{ role: "user", content: "Hello!" }]
});

console.log(response.choices[0].message.content);
```

### For cURL

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "What is AI?"}]
  }'
```

---

## üìä Step 5: Monitor Your Savings (Real-time)

### Via Dashboard (Easiest)

1. Open http://localhost:3000
2. Watch the metrics update in real-time
3. See your cost savings grow with each request

### Via API

```bash
# Get analytics
curl -H "X-API-Key: your_secret_leo_api_key" \
  http://localhost:8000/v1/analytics | jq '.'

# Expected response:
{
  "optimization_metrics": {
    "total_requests": 1250,
    "cache_hits": 450,
    "cache_hit_rate": 0.36,
    "cost_saved": 45.50,
    "tokens_saved": 12500,
    "average_confidence": 0.87
  }
}
```

### Via Python

```python
import requests
import json

response = requests.get(
    "http://localhost:8000/v1/analytics",
    headers={"X-API-Key": "your_secret_leo_api_key"}
)

data = response.json()
print(f"Cost Saved: ${data['optimization_metrics']['cost_saved']}")
print(f"Cache Hit Rate: {data['optimization_metrics']['cache_hit_rate']*100:.1f}%")
print(f"Tokens Saved: {data['optimization_metrics']['tokens_saved']:,}")
```

---

## üéØ How LEO Optima Works (Behind the Scenes)

When you send a query to LEO Optima, here's what happens:

```
1. Your Request
   ‚Üì
2. Request Deduplication
   ‚Üí If identical request is being processed, wait for result (FREE!)
   ‚Üì
3. Cache Lookup (Redis + SQLite)
   ‚Üí If similar query exists in cache, return cached answer (FREE!)
   ‚Üì
4. Query Decomposition
   ‚Üí Break complex queries into simpler parts (20-30% token savings)
   ‚Üì
5. Prompt Optimization
   ‚Üí Remove unnecessary words, optimize for efficiency
   ‚Üì
6. Send to OpenAI
   ‚Üí Only if not cached or decomposed
   ‚Üì
7. Confidence Scoring
   ‚Üí Verify response quality (retry if < 60% confidence)
   ‚Üì
8. Cache Storage
   ‚Üí Store answer for future use
   ‚Üì
9. Return Response + Metrics
   ‚Üí Your application gets the answer + optimization details
```

**Result: 60-80% cost reduction while maintaining quality**

---

## üìà Expected Results Timeline

### After 1 Day
- Cache size: 5-10 entries
- Cache hit rate: 5-10%
- Cost savings: 1-2%

### After 1 Week
- Cache size: 50-100 entries
- Cache hit rate: 15-25%
- Cost savings: 10-20%

### After 1 Month
- Cache size: 200-500 entries
- Cache hit rate: 30-40%
- Cost savings: 50-70%

### After 3 Months
- Cache size: 500-1000+ entries
- Cache hit rate: 40-50%
- Cost savings: 60-80%

---

## üîê Multi-Tenant Management

If you're managing multiple users/applications:

### Create a New Tenant

```bash
curl -X POST http://localhost:8000/v1/admin/tenants \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{
    "name": "Customer A",
    "tier": "pro",
    "token_quota": 1000000,
    "cost_limit": 100
  }'

# Response:
{
  "tenant_id": "tenant_abc123",
  "api_key": "leo_tenant_abc123_xyz789",
  "name": "Customer A",
  "tier": "pro"
}
```

### List All Tenants

```bash
curl -H "X-API-Key: your_secret_leo_api_key" \
  http://localhost:8000/v1/admin/tenants | jq '.'
```

### View Tenant Analytics

Each tenant's analytics are isolated:

```bash
curl -H "X-API-Key: tenant_specific_api_key" \
  http://localhost:8000/v1/analytics | jq '.'
```

---

## ‚öôÔ∏è Configuration & Customization

### Enable/Disable Optimizations

```bash
# Disable cache (for testing)
curl -X POST http://localhost:8000/v1/optimization/enable \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{"strategy": "adaptive_cache", "enabled": false}'

# Disable query decomposition
curl -X POST http://localhost:8000/v1/optimization/enable \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{"strategy": "query_decomposition", "enabled": false}'

# Check current status
curl -H "X-API-Key: your_secret_leo_api_key" \
  http://localhost:8000/v1/optimization/status | jq '.config'
```

### Adjust Cache Threshold

```bash
# Lower threshold = more cache hits but potentially lower quality
# Higher threshold = fewer cache hits but higher quality

curl -X POST http://localhost:8000/v1/optimization/cache/threshold \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{"threshold": 0.85}'  # 0.0 to 1.0
```

---

## üêõ Troubleshooting

### Issue: "Connection refused" when accessing dashboard

**Solution:**
```bash
# Check if services are running
docker compose ps

# If not running, start them
docker compose up -d

# Check logs for errors
docker compose logs leo-optima
```

### Issue: "Invalid API Key" error

**Solution:**
```bash
# Verify your LEO_API_KEY is set correctly
echo $LEO_API_KEY

# Make sure you're using the same key in your requests
# Check .env file for the correct key
cat .env | grep LEO_API_KEY
```

### Issue: High costs, low cache hit rate

**Solution:**
```bash
# Check cache statistics
curl -H "X-API-Key: your_secret_leo_api_key" \
  http://localhost:8000/v1/optimization/cache/stats

# Enable all optimizations
curl -X POST http://localhost:8000/v1/optimization/enable \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{"strategy": "adaptive_cache", "enabled": true}'

# Lower cache threshold for more hits
curl -X POST http://localhost:8000/v1/optimization/cache/threshold \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{"threshold": 0.75}'
```

### Issue: Dashboard shows no data

**Solution:**
```bash
# Make sure you've sent some requests through LEO
# Send a test request
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your_secret_leo_api_key" \
  -d '{
    "model": "gpt-4",
    "messages": [{"role": "user", "content": "Hello"}]
  }'

# Wait a few seconds, then refresh dashboard
# http://localhost:3000
```

---

## üéì Best Practices

### 1. Use Consistent Models

```python
# ‚úÖ Good: Always use the same model
client.chat.completions.create(model="gpt-4", ...)

# ‚ùå Bad: Switching between models reduces cache effectiveness
client.chat.completions.create(model="gpt-4", ...)
client.chat.completions.create(model="gpt-3.5-turbo", ...)
```

### 2. Batch Similar Queries

```python
# ‚úÖ Good: Process similar queries together
queries = [
    "What is machine learning?",
    "What is deep learning?",
    "What is neural networks?"
]
for query in queries:
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": query}]
    )
```

### 3. Monitor Cache Performance

```bash
# Weekly check
curl -H "X-API-Key: your_secret_leo_api_key" \
  http://localhost:8000/v1/analytics | jq '{
    cache_hit_rate: .optimization_metrics.cache_hit_rate,
    cost_saved: .optimization_metrics.cost_saved,
    tokens_saved: .optimization_metrics.tokens_saved
  }'
```

### 4. Provide Feedback

```python
# Help LEO learn what's correct
feedback = {
    "cached_answer": "Machine learning is...",
    "is_correct": True  # or False
}

requests.post(
    "http://localhost:8000/v1/optimization/cache/feedback",
    json=feedback,
    headers={"X-API-Key": "your_secret_leo_api_key"}
)
```

---

## üìö Next Steps

1. **Read the Full Documentation**
   - [README.md](./README.md) - Project overview
   - [DASHBOARD.md](./DASHBOARD.md) - Dashboard guide
   - [API_DOCUMENTATION.md](./API_DOCUMENTATION.md) - Complete API reference

2. **Explore Advanced Features**
   - Multi-tenant management
   - Custom optimization strategies
   - Webhook integration
   - Audit logging

3. **Monitor Your Savings**
   - Check dashboard daily: http://localhost:3000
   - Review analytics weekly
   - Adjust settings based on performance

4. **Integrate Fully**
   - Replace all OpenAI calls with LEO
   - Set up monitoring alerts
   - Configure backup strategies

---

## üÜò Support & Issues

If you encounter problems:

1. **Check the logs**
   ```bash
   docker compose logs leo-optima
   ```

2. **Review the troubleshooting section** above

3. **Check GitHub Issues**
   - https://github.com/BADJAB22/leo-optima/issues

4. **Read the documentation**
   - [API_DOCUMENTATION.md](./API_DOCUMENTATION.md)
   - [TECHNICAL_ANALYSIS.md](./TECHNICAL_ANALYSIS.md)

---

## üéâ You're Ready!

You now have a fully functional LLM cost-reduction engine. Start seeing savings immediately!

**Quick Summary:**
1. ‚úÖ Cloned the repository
2. ‚úÖ Set up environment variables
3. ‚úÖ Started LEO Optima with Docker
4. ‚úÖ Accessed the dashboard
5. ‚úÖ Connected your application
6. ‚úÖ Monitoring your savings

**Happy optimizing!** üöÄ
